{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\anaconda\\lib\\site-packages (4.0.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\anaconda\\lib\\site-packages (from gensim) (1.19.2)\n",
      "Requirement already satisfied: Cython==0.29.21 in c:\\anaconda\\lib\\site-packages (from gensim) (0.29.21)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\anaconda\\lib\\site-packages (from gensim) (5.0.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\anaconda\\lib\\site-packages (from gensim) (1.5.2)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.0.1; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the 'C:\\Anaconda\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: xgboost in c:\\anaconda\\lib\\site-packages (1.4.1)\n",
      "Requirement already satisfied: numpy in c:\\anaconda\\lib\\site-packages (from xgboost) (1.19.2)\n",
      "Requirement already satisfied: scipy in c:\\anaconda\\lib\\site-packages (from xgboost) (1.5.2)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.0.1; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the 'C:\\Anaconda\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alias_setup(probs):\n",
    "    \"\"\"\n",
    "    compute utility lists for non-uniform sampling from discrete distributions.\n",
    "    details: https://lips.cs.princeton.edu/the-alias-method-efficient-sampling-with-many-discrete-outcomes/\n",
    "    :param probs: 某个概率分布\n",
    "    \"\"\"\n",
    "    K = len(probs) # K为类型数目\n",
    "    q = np.zeros(K) # 对应q数组：落在原类型的概率\n",
    "    J = np.zeros(K, dtype=np.int) # 对应J数组：每一列第二层的类型\n",
    "    \n",
    "    #Sort the data into the outcomes with probabilities\n",
    "    #that are larger and smaller than 1/K\n",
    "    smaller = list() # 存储比1小的列\n",
    "    larger = list() # 存储比1大的列\n",
    "    \n",
    "    for kk, prob in enumerate(probs):\n",
    "        q[kk] = K * prob # 概率（每个类别概率乘以K，使得总和为K）\n",
    "        if q[kk] < 1.0: # 然后分为两类：大于1的和小于1的\n",
    "            smaller.append(kk)\n",
    "        else:\n",
    "            larger.append(kk)\n",
    "\n",
    "    # 通过拼凑，将各个类别都凑为1\n",
    "    while len(smaller) > 0 and len(larger) > 0:\n",
    "        small = smaller.pop()\n",
    "        large = larger.pop()\n",
    "\n",
    "        J[small] = large\n",
    "        q[large] = q[large] + q[small] - 1.0 #将大的分到小的上\n",
    "        if q[large] < 1.0:\n",
    "            smaller.append(large)\n",
    "        else:\n",
    "            larger.append(large)\n",
    "\n",
    "    return J, q\n",
    "\n",
    "def get_alias_node(graph, node):\n",
    "    \"\"\"\n",
    "    get the alias node setup lists for a given node.\n",
    "    \"\"\"\n",
    "    # get the unnormalized probabilities with the first-order information\n",
    "    unnormalized_probs = list()\n",
    "    for nbr in graph.neighbors(node):\n",
    "        unnormalized_probs.append(graph[node][nbr][\"weight\"])\n",
    "    unnormalized_probs = np.array(unnormalized_probs)\n",
    "    if len(unnormalized_probs) > 0:\n",
    "        normalized_probs = unnormalized_probs / unnormalized_probs.sum()\n",
    "    else:\n",
    "        normalized_probs = unnormalized_probs\n",
    "        \n",
    "    return alias_setup(normalized_probs)\n",
    "    \n",
    "def get_alias_edge(graph, src, dst, p=1, q=1):\n",
    "    \"\"\"\n",
    "    get the alias edge setup lists for a given edge.\n",
    "    \"\"\"\n",
    "    # get the unnormalized probabilities with the second-order information\n",
    "    unnormalized_probs = list()\n",
    "    for dst_nbr in graph.neighbors(dst):\n",
    "        if dst_nbr == src: # distance is 0\n",
    "            unnormalized_probs.append(graph[dst][dst_nbr][\"weight\"]/p)\n",
    "        elif graph.has_edge(dst_nbr, src): # distance is 1\n",
    "            unnormalized_probs.append(graph[dst][dst_nbr][\"weight\"])\n",
    "        else: # distance is 2\n",
    "            unnormalized_probs.append(graph[dst][dst_nbr][\"weight\"]/q)\n",
    "    unnormalized_probs = np.array(unnormalized_probs)\n",
    "    if len(unnormalized_probs) > 0:\n",
    "        normalized_probs = unnormalized_probs / unnormalized_probs.sum()\n",
    "    else:\n",
    "        normalized_probs = unnormalized_probs\n",
    "\n",
    "    return alias_setup(normalized_probs)\n",
    "\n",
    "def preprocess_transition_probs(graph, p=1, q=1):\n",
    "    \"\"\"\n",
    "    preprocess transition probabilities for guiding the random walks.\n",
    "    \"\"\"\n",
    "    alias_nodes = dict()\n",
    "    for node in graph.nodes():\n",
    "        alias_nodes[node] = get_alias_node(graph, node)\n",
    "\n",
    "    alias_edges = dict()\n",
    "    for edge in graph.edges():\n",
    "        alias_edges[edge] = get_alias_edge(graph, edge[0], edge[1], p=p, q=q)\n",
    "\n",
    "    return alias_nodes, alias_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alias_draw(J, q):\n",
    "    \"\"\"\n",
    "    draw sample from a non-uniform discrete distribution using alias sampling.\n",
    "    \"\"\"\n",
    "    K = len(J)\n",
    "\n",
    "    kk = int(np.floor(np.random.rand() * K))\n",
    "    if np.random.rand() < q[kk]:\n",
    "        return kk\n",
    "    else:\n",
    "        return J[kk]\n",
    "\n",
    "\n",
    "# helper function to generate the long random walk as desired\n",
    "def fallback(walk, fetch_last_num=1):\n",
    "    if len(walk) > fetch_last_num:\n",
    "        walk.pop()\n",
    "        fetched = []\n",
    "        for i in range(fetch_last_num):\n",
    "            fetched.append(walk[-1-i])\n",
    "        return walk, fetched\n",
    "    else:\n",
    "        return [], [None for _ in range(fetch_last_num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    \"\"\"\n",
    "    read edges from an edge file\n",
    "    \"\"\"\n",
    "    edges = list()\n",
    "    df = pd.read_csv(file_name)\n",
    "    for idx, row in df.iterrows():\n",
    "        user_id, friends = row[\"user_id\"], eval(row[\"friends\"])\n",
    "        for friend in friends:\n",
    "            # add each friend relation as an edge\n",
    "            edges.append((user_id, friend))\n",
    "    edges = sorted(edges)\n",
    "    print(\"The number of edges is\", len(edges))\n",
    "    nodes = list(set(chain.from_iterable(edges)))\n",
    "    print(\"The number of nodes is\", len(nodes))\n",
    "    return edges, nodes\n",
    "\n",
    "def load_test_data(file_name):\n",
    "    \"\"\"\n",
    "    read edges from an edge file\n",
    "    \"\"\"\n",
    "    edges = list()\n",
    "    scores = list()\n",
    "    df = pd.read_csv(file_name)\n",
    "    for idx, row in df.iterrows():\n",
    "        edges.append((row[\"src\"], row[\"dst\"]))\n",
    "    edges = sorted(edges)\n",
    "    print(\"The number of edges is\", len(edges))\n",
    "    nodes = list(set(chain.from_iterable(edges)))\n",
    "    print(\"The number of nodes is\", len(nodes))\n",
    "    return edges\n",
    "\n",
    "def generate_false_edges(true_edges, num_false_edges=5):\n",
    "    \"\"\"\n",
    "    generate false edges given true edges\n",
    "    \"\"\"\n",
    "    # chain.from_iterable 将所有edges的nodes合成一个iterable，set去重后转为list\n",
    "    nodes = list(set(chain.from_iterable(true_edges)))\n",
    "    N = len(nodes)\n",
    "    true_edges = set(true_edges) # true_edges去重\n",
    "    print(\"The number of nodes and edges before false edges generation:\", N, len(true_edges)) #输出 nodes数量，edges数量\n",
    "    false_edges = set()\n",
    "    \n",
    "    while len(false_edges) < num_false_edges:\n",
    "        # randomly sample two different nodes and check whether the pair exisit or not\n",
    "        src, dst = nodes[int(np.random.rand() * N)], nodes[int(np.random.rand() * N)]\n",
    "        # 去掉 A->A 类的自指 edge && 不在 true_edges 中 && 与之前 false_edges 不重复\n",
    "        if src != dst and (src, dst) not in true_edges and (src, dst) not in false_edges:\n",
    "            false_edges.add((src, dst))\n",
    "    false_edges = sorted(false_edges)\n",
    "    print(\"Generated \", len(false_edges),'false edges...')\n",
    "    return false_edges\n",
    "\n",
    "def construct_graph_from_edges(edges):\n",
    "    \"\"\"\n",
    "    generate a directed graph object given true edges\n",
    "    DiGraph documentation: https://networkx.github.io/documentation/stable/reference/classes/digraph.html\n",
    "    \"\"\"\n",
    "    # convert a list of edges {(u, v)} to a list of edges with weights {(u, v, w)}\n",
    "    edge_weight = defaultdict(float)\n",
    "    \n",
    "    # edge_weight 由重复次数决定？\n",
    "    for e in edges:\n",
    "        edge_weight[e] += 1.0\n",
    "    weighed_edge_list = list()\n",
    "    for e in sorted(edge_weight.keys()):\n",
    "        weighed_edge_list.append((e[0], e[1], edge_weight[e]))\n",
    "        \n",
    "    graph = nx.DiGraph()\n",
    "    graph.add_weighted_edges_from(weighed_edge_list)\n",
    "    \n",
    "    print(\"number of nodes:\", graph.number_of_nodes())\n",
    "    print(\"number of edges:\", graph.number_of_edges())\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_second_order_random_walk(graph, alias_nodes, alias_edges, \n",
    "                                      walk_length=10, start_node=None, verbose=False, max_trails=10):\n",
    "    \"\"\"\n",
    "    simulate a random walk starting from start node and considering the second order information.\n",
    "    \"\"\"\n",
    "    if start_node == None:\n",
    "        start_node = np.random.choice(graph.nodes())\n",
    "    walk = [start_node]\n",
    "    \n",
    "    prev = None\n",
    "    cur = start_node\n",
    "    num_tried = 0\n",
    "\n",
    "    ########## begin ##########\n",
    "    while len(walk) < walk_length:\n",
    "        cur_nbrs = list(graph.neighbors(cur))\n",
    "        if len(cur_nbrs) > 0:\n",
    "            if prev is None:\n",
    "                # sample the next node based on alias_nodes\n",
    "                prev, cur = cur, cur_nbrs[alias_draw(*alias_nodes[cur])]\n",
    "            else:\n",
    "                # sample the next node based on alias_edges\n",
    "                prev, cur = cur, cur_nbrs[alias_draw(*alias_edges[(prev, cur)])]\n",
    "            walk.append(cur)\n",
    "        else:\n",
    "            num_tried += 1\n",
    "            if num_tried >= max_trails:\n",
    "                break\n",
    "            walk, (cur, prev) = fallback(walk, fetch_last_num=2)\n",
    "            if len(walk) == 0:\n",
    "                start_node = np.random.choice(graph.nodes())\n",
    "                walk = [start_node]\n",
    "                cur = start_node\n",
    "                prev = None\n",
    "    ########## end ##########\n",
    "    if verbose: \n",
    "        print(f'walk of lenght {len(walk)} generated with {num_tried} trails')\n",
    "    return walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_node2vec(graph, alias_nodes, alias_edges, node_dim=10, num_walks=10, walk_length=10):\n",
    "    \"\"\"\n",
    "    build a node2vec model\n",
    "    \"\"\"\n",
    "    print(\"\\nbuilding a node2vec model...\", end=\"\\t\")\n",
    "    st = time.time()\n",
    "    np.random.seed(0)\n",
    "    nodes = list(graph.nodes())\n",
    "    walks = list()\n",
    "    # generate random walks\n",
    "    for walk_iter in range(num_walks):\n",
    "        np.random.shuffle(nodes)\n",
    "        for node in nodes:\n",
    "            walks.append(generate_second_order_random_walk(\n",
    "                graph, alias_nodes, alias_edges, walk_length=walk_length, start_node=node))\n",
    "            \n",
    "    walk_lens = [len(w) for w in walks]\n",
    "    if len(walk_lens) > 0:\n",
    "        avg_walk_len = sum(walk_lens) / len(walk_lens)\n",
    "    else:\n",
    "        avg_walk_len = 0.0    \n",
    "    print(\"number of walks: %d\\taverage walk length: %.4f\" % (len(walks), avg_walk_len), end=\"\\t\")\n",
    "    \n",
    "    # train a skip-gram model for these walks\n",
    "    model = Word2Vec(walks, vector_size=node_dim, window=3, min_count=0, sg=1, workers=os.cpu_count(), epochs=10)\n",
    "    print(\"training time: %.4f\" % (time.time()-st))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_sim(model, u, v):\n",
    "    \"\"\"\n",
    "    get the cosine similarity between two nodes\n",
    "    \"\"\"\n",
    "    try:\n",
    "        u = model.wv[u]\n",
    "        v = model.wv[v]\n",
    "        return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
    "    except:\n",
    "        return 0.5\n",
    "\n",
    "def get_auc_score(model, true_edges, false_edges):\n",
    "    \"\"\"\n",
    "    get the auc score\n",
    "    \"\"\"\n",
    "    y_true = [1] * len(true_edges) + [0] * len(false_edges)\n",
    "    \n",
    "    y_score = list()\n",
    "    for e in true_edges:\n",
    "        sim = get_cosine_sim(model, e[0], e[1])\n",
    "        y_score.append(sim)\n",
    "    for e in false_edges:\n",
    "        sim = get_cosine_sim(model, e[0], e[1])\n",
    "        y_score.append(sim)\n",
    "    \n",
    "    return roc_auc_score(y_true, y_score)\n",
    "\n",
    "def write_pred(file_name, edges, scores):\n",
    "    df = pd.DataFrame()\n",
    "    df[\"src\"] = [e[0] for e in edges]\n",
    "    df[\"dst\"] = [e[1] for e in edges]\n",
    "    df[\"score\"] = scores\n",
    "    df.to_csv(file_name, index=False)\n",
    "    \n",
    "\n",
    "def write_valid_ans(file_name, edges, scores):\n",
    "    df = pd.DataFrame()\n",
    "    df[\"src\"] = [e[0] for e in edges]\n",
    "    df[\"dst\"] = [e[1] for e in edges]\n",
    "    df[\"score\"] = scores\n",
    "    df.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load train edges...\n",
      "The number of edges is 100000\n",
      "The number of nodes is 8328\n",
      "\n",
      "Generate graph...\n",
      "number of nodes: 8328\n",
      "number of edges: 100000\n",
      "\n",
      "Load valid edges...\n",
      "The number of edges is 19268\n",
      "The number of nodes is 5440\n",
      "\n",
      "Generate train_false edges...\n",
      "The number of nodes and edges before false edges generation: 8328 100000\n",
      "Generated  100000 false edges...\n",
      "\n",
      "Generate valid_false edges...\n",
      "The number of nodes and edges before false edges generation: 8474 119268\n",
      "Generated  20732 false edges...\n",
      "\n",
      "Load test edges...\n",
      "The number of edges is 40000\n",
      "The number of nodes is 8509\n"
     ]
    }
   ],
   "source": [
    "train_file = \"data/train.csv\"\n",
    "valid_file = \"data/valid.csv\"\n",
    "test_file = \"data/test.csv\"\n",
    "\n",
    "np.random.seed(0)\n",
    "print(\"Load train edges...\")\n",
    "train_edges, train_nodes = load_data(train_file)\n",
    "print(\"\\nGenerate graph...\")\n",
    "graph = construct_graph_from_edges(train_edges)\n",
    "print(\"\\nLoad valid edges...\")\n",
    "valid_edges,valid_nodes = load_data(valid_file)\n",
    "print(\"\\nGenerate train_false edges...\")\n",
    "train_false_edges = generate_false_edges(train_edges, 100000)\n",
    "print(\"\\nGenerate valid_false edges...\")\n",
    "false_edges = generate_false_edges(train_edges+valid_edges, 40000-len(valid_edges))\n",
    "print(\"\\nLoad test edges...\")\n",
    "test_edges = load_test_data(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node dim: 10,\tnum_walks: 10,\twalk_length: 10,\tp: 0.50,\tq: 0.50\t\n",
      "building a node2vec model...\tnumber of walks: 83280\taverage walk length: 9.9918\ttraining time: 25.1930\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "node_dim = 10\n",
    "num_walks = 10\n",
    "walk_length = 10\n",
    "p = 0.5\n",
    "q = 0.5\n",
    "\n",
    "print(\"node dim: %d,\\tnum_walks: %d,\\twalk_length: %d,\\tp: %.2f,\\tq: %.2f\" % (\n",
    "    node_dim, num_walks, walk_length, p, q), end=\"\\t\")\n",
    "\n",
    "alias_nodes, alias_edges = preprocess_transition_probs(graph, p=p, q=q)\n",
    "model = build_node2vec(graph, alias_nodes, alias_edges, \n",
    "                       node_dim=node_dim, num_walks=num_walks, walk_length=walk_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7969"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(node):\n",
    "    try:\n",
    "        node_emd = model.wv[node]\n",
    "        return np.array(node_emd)\n",
    "    except:\n",
    "        return avg_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vec = np.zeros((8328,10))\n",
    "i = -1\n",
    "\n",
    "for node in model.wv.index_to_key:\n",
    "    i += 1\n",
    "    w_vec = model.wv[node]\n",
    "    all_vec[i] = w_vec\n",
    "\n",
    "avg_vec = np.mean(all_vec,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_data=np.zeros((200000,10))\n",
    "i = -1\n",
    "for (a,b) in train_edges:\n",
    "    i += 1\n",
    "    add_list = get_embedding(a)+get_embedding(b)\n",
    "    em_data[i] = add_list\n",
    "    \n",
    "i = 99999\n",
    "for (a,b) in train_false_edges:\n",
    "    i += 1\n",
    "    add_list = get_embedding(a)+get_embedding(b)\n",
    "    em_data[i] = add_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain=[]\n",
    "for i in range(100000):\n",
    "    ytrain.append(1)\n",
    "for i in range(100000):\n",
    "    ytrain.append(0)\n",
    "ytrain=np.array(ytrain)\n",
    "ytrain.reshape((200000,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 10)\n",
      "(200000,)\n"
     ]
    }
   ],
   "source": [
    "print(em_data.shape)\n",
    "print(ytrain.shape)\n",
    "data = pd.DataFrame(em_data)\n",
    "lable = pd.DataFrame(ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:06:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(alpha=10, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints='',\n",
       "             learning_rate=0.5, max_delta_step=0, max_depth=20,\n",
       "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=100, n_jobs=16, num_parallel_tree=1,\n",
       "             objective='binary:logistic', random_state=0, reg_alpha=10,\n",
       "             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact',\n",
       "             validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "dtrain = xgb.DMatrix(data, label=lable)\n",
    "\n",
    "xg_reg = xgb.XGBRegressor(objective ='binary:logistic', colsample_bytree = 0.3, learning_rate = 0.5,\n",
    "                max_depth = 20, alpha = 10, n_estimators = 100)\n",
    "xg_reg.fit(data,lable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_valid_data=np.zeros((40000,10))\n",
    "i=-1\n",
    "for (a,b) in valid_edges:\n",
    "    i+=1\n",
    "    add_list = get_embedding(a)+get_embedding(b)\n",
    "    em_valid_data[i] = add_list\n",
    "    \n",
    "i=19267\n",
    "for (a,b) in false_edges:\n",
    "    i+=1\n",
    "    add_list = get_embedding(a)+get_embedding(b)\n",
    "    em_valid_data[i] = add_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yvalid=[]\n",
    "for i in range(19268):\n",
    "    yvalid.append(1)\n",
    "for i in range(eval(\"40000-19268\")):\n",
    "    yvalid.append(0)\n",
    "yvalid=np.array(yvalid)\n",
    "yvalid.reshape((40000,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 10)\n",
      "(40000,)\n"
     ]
    }
   ],
   "source": [
    "print(em_valid_data.shape)\n",
    "print(yvalid.shape)\n",
    "em_valid_data = pd.DataFrame(em_valid_data)\n",
    "valid_lable = pd.DataFrame(yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8161394 , 0.07216774, 0.70945174, ..., 0.00217352, 0.11858444,\n",
       "       0.00255821], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = xg_reg.predict(em_valid_data)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9599688721523806\n"
     ]
    }
   ],
   "source": [
    "auc_scores=roc_auc_score(yvalid, preds)\n",
    "print (auc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node dim: 10,\tnum_walks: 5,\twalk_length: 10,\tp: 0.50,\tq: 0.50\t\n",
      "building a node2vec model...\tnumber of walks: 41640\taverage walk length: 9.9919\ttraining time: 9.6020\n",
      "[19:48:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "node dim: 10,\tnum_walks: 5,\twalk_length: 20,\tp: 0.50,\tq: 0.50\t\n",
      "building a node2vec model...\tnumber of walks: 41640\taverage walk length: 19.9671\ttraining time: 14.2092\n",
      "[19:49:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "node dim: 10,\tnum_walks: 5,\twalk_length: 40,\tp: 0.50,\tq: 0.50\t\n",
      "building a node2vec model...\tnumber of walks: 41640\taverage walk length: 39.8780\ttraining time: 23.5425\n",
      "[19:50:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "node dim: 10,\tnum_walks: 10,\twalk_length: 10,\tp: 0.50,\tq: 0.50\t\n",
      "building a node2vec model...\tnumber of walks: 83280\taverage walk length: 9.9918\ttraining time: 19.1082\n",
      "[19:51:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "node dim: 10,\tnum_walks: 10,\twalk_length: 20,\tp: 0.50,\tq: 0.50\t\n",
      "building a node2vec model...\tnumber of walks: 83280\taverage walk length: 19.9665\ttraining time: 32.0936\n",
      "[19:53:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "node dim: 10,\tnum_walks: 10,\twalk_length: 40,\tp: 0.50,\tq: 0.50\t\n",
      "building a node2vec model...\tnumber of walks: 83280\taverage walk length: 39.8755\ttraining time: 51.5372\n",
      "[19:55:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "node dim: 10,\tnum_walks: 20,\twalk_length: 10,\tp: 0.50,\tq: 0.50\t\n",
      "building a node2vec model...\tnumber of walks: 166560\taverage walk length: 9.9915\ttraining time: 40.3809\n",
      "[19:56:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "node dim: 10,\tnum_walks: 20,\twalk_length: 20,\tp: 0.50,\tq: 0.50\t\n",
      "building a node2vec model...\tnumber of walks: 166560\taverage walk length: 19.9677\ttraining time: 56.5725\n",
      "[19:58:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "node dim: 10,\tnum_walks: 20,\twalk_length: 40,\tp: 0.50,\tq: 0.50\t\n",
      "building a node2vec model...\tnumber of walks: 166560\taverage walk length: 39.8758\ttraining time: 92.2277\n",
      "[20:00:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "node dim: 10,\tnum_walks: 40,\twalk_length: 10,\tp: 0.50,\tq: 0.50\t\n",
      "building a node2vec model...\tnumber of walks: 333120\taverage walk length: 9.9912\ttraining time: 74.6957\n",
      "[20:03:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "node dim: 10,\tnum_walks: 40,\twalk_length: 20,\tp: 0.50,\tq: 0.50\t\n",
      "building a node2vec model...\tnumber of walks: 333120\taverage walk length: 19.9678\ttraining time: 113.5359\n",
      "[20:05:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "node dim: 10,\tnum_walks: 40,\twalk_length: 40,\tp: 0.50,\tq: 0.50\t\n",
      "building a node2vec model...\tnumber of walks: 333120\taverage walk length: 39.8776\ttraining time: 183.3326\n",
      "[20:09:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "          5         10        20        40\n",
      "10  0.964782  0.959141  0.960523  0.965694\n",
      "20  0.959553  0.961295  0.964317  0.964535\n",
      "40  0.959810  0.961637  0.964211  0.964695\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(np.random.randn(3, 4),\n",
    "                index=[10, 20, 40], columns=[5, 10, 20, 40])\n",
    "for num_walk in [5, 10, 20, 40]:\n",
    "    for walk_len in  [10, 20, 40]:\n",
    "        np.random.seed(0)\n",
    "\n",
    "        node_dim = 10\n",
    "        num_walks = num_walk\n",
    "        walk_length = walk_len\n",
    "        p = 0.5\n",
    "        q = 0.5\n",
    "\n",
    "        print(\"node dim: %d,\\tnum_walks: %d,\\twalk_length: %d,\\tp: %.2f,\\tq: %.2f\" % (\n",
    "            node_dim, num_walks, walk_length, p, q), end=\"\\t\")\n",
    "\n",
    "        alias_nodes, alias_edges = preprocess_transition_probs(graph, p=p, q=q)\n",
    "        model = build_node2vec(graph, alias_nodes, alias_edges, \n",
    "                               node_dim=node_dim, num_walks=num_walks, walk_length=walk_length)\n",
    "\n",
    "        def get_embedding(node):\n",
    "            try:\n",
    "                node_emd = model.wv[node]\n",
    "                return np.array(node_emd)\n",
    "            except:\n",
    "                return avg_vec\n",
    "\n",
    "        all_vec = np.zeros((8328,10))\n",
    "        i = -1\n",
    "\n",
    "        for node in model.wv.index_to_key:\n",
    "            i += 1\n",
    "            w_vec = model.wv[node]\n",
    "            all_vec[i] = w_vec\n",
    "\n",
    "        avg_vec = np.mean(all_vec,axis=0)\n",
    "\n",
    "        em_data=np.zeros((200000,10))\n",
    "        i = -1\n",
    "        for (a,b) in train_edges:\n",
    "            i += 1\n",
    "            add_list = get_embedding(a)+get_embedding(b)\n",
    "            em_data[i] = add_list\n",
    "\n",
    "        i = 99999\n",
    "        for (a,b) in train_false_edges:\n",
    "            i += 1\n",
    "            add_list = get_embedding(a)+get_embedding(b)\n",
    "            em_data[i] = add_list\n",
    "\n",
    "        ytrain=[]\n",
    "        for i in range(100000):\n",
    "            ytrain.append(1)\n",
    "        for i in range(100000):\n",
    "            ytrain.append(0)\n",
    "        ytrain=np.array(ytrain)\n",
    "        ytrain.reshape((200000,-1))\n",
    "\n",
    "        data = pd.DataFrame(em_data)\n",
    "        lable = pd.DataFrame(ytrain)\n",
    "\n",
    "        dtrain = xgb.DMatrix(data, label=lable)\n",
    "        xg_reg = xgb.XGBRegressor(objective ='binary:logistic', colsample_bytree = 0.3, learning_rate = 0.5,\n",
    "                        max_depth = 20, alpha = 10, n_estimators = 100)\n",
    "        xg_reg.fit(data,lable)\n",
    "\n",
    "        em_valid_data=np.zeros((40000,10))\n",
    "        i=-1\n",
    "        for (a,b) in valid_edges:\n",
    "            i+=1\n",
    "            add_list = get_embedding(a)+get_embedding(b)\n",
    "            em_valid_data[i] = add_list\n",
    "\n",
    "        i=19267\n",
    "        for (a,b) in false_edges:\n",
    "            i+=1\n",
    "            add_list = get_embedding(a)+get_embedding(b)\n",
    "            em_valid_data[i] = add_list\n",
    "\n",
    "        yvalid=[]\n",
    "        for i in range(19268):\n",
    "            yvalid.append(1)\n",
    "        for i in range(eval(\"40000-19268\")):\n",
    "            yvalid.append(0)\n",
    "        yvalid=np.array(yvalid)\n",
    "        yvalid.reshape((40000,-1))\n",
    "\n",
    "        em_valid_data = pd.DataFrame(em_valid_data)\n",
    "        valid_lable = pd.DataFrame(yvalid)\n",
    "\n",
    "        preds = xg_reg.predict(em_valid_data)\n",
    "\n",
    "        auc_scores = roc_auc_score(yvalid, preds)\n",
    "        df.loc[walk_length,num_walks] = auc_scores\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          5         10        20        40\n",
      "10  0.964782  0.959141  0.960523  0.965694\n",
      "20  0.959553  0.961295  0.964317  0.964535\n",
      "40  0.959810  0.961637  0.964211  0.964695\n"
     ]
    }
   ],
   "source": [
    "print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'num_walks')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(3, 4))\n",
    "node_dim = 10\n",
    "# you should have an auc score dictionary here.\n",
    "a = np.array( [ (0.964782  ,0.959141  ,0.960523 ,0.965694 ),\n",
    "            (0.959553  ,0.961295  ,0.964317 ,0.964535 ),\n",
    "           (0.959810,0.961637  ,0.964211  ,0.964695  )] )\n",
    "plt.imshow(a, cmap=\"hot\", interpolation=\"nearest\")\n",
    "plt.colorbar()\n",
    "plt.yticks(ticks=[0,1,2], labels=[40, 20, 10])\n",
    "plt.ylabel(\"walk_length\")\n",
    "plt.xticks(ticks=[0,1,2,3], labels=[5, 10, 20, 40])\n",
    "plt.xlabel(\"num_walks\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
